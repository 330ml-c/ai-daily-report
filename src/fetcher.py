"""
GitHub AI é¡¹ç›®æ•°æ®æŠ“å–æ¨¡å—
è´Ÿè´£ä» GitHub API è·å–æ´»è·ƒçš„ AI ç›¸å…³é¡¹ç›®
"""

import os
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
import requests

try:
    from cache import StarCache
except ImportError:
    from src.cache import StarCache


class GitHubFetcher:
    """GitHub é¡¹ç›®æ•°æ®æŠ“å–å™¨"""

    # é€šé“åŒ–æ£€ç´¢é…ç½®ï¼šèšç„¦ vibecodingã€codexã€claude ä¸‰ä¸ªä¸»é¢˜
    SEARCH_CHANNELS = {
        "vibecoding": [
            '(vibecoding OR "vibe coding") in:name,description,readme archived:false stars:>10',
            'topic:vibecoding archived:false stars:>10',
        ],
        "codex": [
            '(codex OR "openai codex") in:name,description,readme archived:false stars:>10',
            'topic:codex archived:false stars:>10',
        ],
        "claude": [
            '(claude OR "claude code") in:name,description,readme archived:false stars:>10',
            '(topic:claude OR topic:anthropic) archived:false stars:>10',
        ],
    }

    # ä¼˜å…ˆçº§è¯„åˆ†æƒé‡
    PRIORITY_WEIGHTS = {
        "relevance": 0.55,
        "growth": 0.30,
        "quality": 0.15,
    }
    CHANNEL_ORDER = ["vibecoding", "codex", "claude"]
    CHANNEL_TOP_N = 15
    REQUEST_TIMEOUT = 15
    MAX_RETRIES = 3

    def __init__(self, github_token: str, cache_dir: str = None):
        """
        åˆå§‹åŒ– GitHub API å®¢æˆ·ç«¯

        Args:
            github_token: GitHub Personal Access Token
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„ star_cache.json
        """
        self.token = github_token
        self.base_url = "https://api.github.com"
        self.headers = {
            "Authorization": f"Bearer {github_token}",
            "Accept": "application/vnd.github+json"
        }

        # åˆå§‹åŒ–ç¼“å­˜
        cache_file = os.path.join(cache_dir, "star_cache.json") if cache_dir else "star_cache.json"
        self.cache = StarCache(cache_file)

    def _search_repositories(self, query: str, per_page: int = 20) -> List[Dict]:
        """
        æœç´¢ä»“åº“

        Args:
            query: æœç´¢æŸ¥è¯¢
            per_page: æ¯é¡µç»“æœæ•°é‡

        Returns:
            ä»“åº“åˆ—è¡¨
        """
        response = self._http_get(
            f"{self.base_url}/search/repositories",
            params={
                "q": query,
                "sort": "updated",
                "order": "desc",
                "per_page": per_page
            },
        )

        if response is None:
            print("  âŒ æœç´¢å¤±è´¥: è¯·æ±‚å¼‚å¸¸ï¼Œå·²é‡è¯•")
            return []

        if response.status_code == 200:
            data = response.json()
            return data.get("items", [])
        else:
            print(f"  âŒ æœç´¢å¤±è´¥: {response.status_code} - {response.text[:100]}")
            return []

    def _http_get(self, url: str, params: Optional[Dict[str, Any]] = None) -> Optional[requests.Response]:
        """
        ç»Ÿä¸€å°è£… GET è¯·æ±‚ï¼Œæä¾›è¶…æ—¶ã€é‡è¯•å’Œé€€é¿èƒ½åŠ›
        """
        response = None
        for attempt in range(self.MAX_RETRIES):
            try:
                response = requests.get(
                    url,
                    params=params,
                    headers=self.headers,
                    timeout=self.REQUEST_TIMEOUT
                )
            except requests.RequestException as e:
                if attempt == self.MAX_RETRIES - 1:
                    print(f"  âŒ è¯·æ±‚å¼‚å¸¸: {e}")
                    return None
                time.sleep(2 ** attempt)
                continue

            if response.status_code == 200:
                return response

            if response.status_code in (403, 429, 500, 502, 503, 504) and attempt < self.MAX_RETRIES - 1:
                retry_seconds = 2 ** attempt
                # GitHub é™æµæ—¶ä¼˜å…ˆæŒ‰é‡ç½®æ—¶é—´ç­‰å¾…ï¼Œé¿å…æ— æ•ˆé‡è¯•
                if response.status_code == 403 and response.headers.get("X-RateLimit-Remaining") == "0":
                    reset_at = response.headers.get("X-RateLimit-Reset")
                    if reset_at:
                        try:
                            retry_seconds = max(1, int(reset_at) - int(time.time()))
                        except ValueError:
                            retry_seconds = 2 ** attempt
                time.sleep(min(retry_seconds, 30))
                continue

            return response

        return response

    def _calculate_star_velocity(self, repo_name: str, repo_data: Dict) -> float:
        """
        è®¡ç®— star å¢é•¿é€Ÿåº¦ï¼ˆæ¯å¤©å¢é•¿æ•°ï¼‰

        ä¼˜å…ˆä½¿ç”¨ç¼“å­˜ä¸­çš„çœŸå®å¢é•¿æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨è¿‘ä¼¼è®¡ç®—

        Args:
            repo_name: é¡¹ç›®å…¨å (owner/repo)
            repo_data: ä»“åº“æ•°æ®å­—å…¸

        Returns:
            star å¢é•¿é€Ÿåº¦ï¼ˆæ¯å¤©å¢é•¿æ•°ï¼‰
        """
        current_stars = repo_data.get("stargazers_count", 0)

        # å°è¯•ä»ç¼“å­˜è·å–çœŸå®å¢é•¿é€Ÿåº¦
        growth_rate = self.cache.calculate_growth_rate(repo_name, current_stars)

        if growth_rate is not None:
            # æœ‰ç¼“å­˜æ•°æ®ï¼Œè¿”å›çœŸå®å¢é•¿é€Ÿåº¦
            print(f"  ğŸ“ˆ {repo_name}: çœŸå®å¢é•¿ {growth_rate:.1f} stars/å¤©")
            return growth_rate

        # æ— ç¼“å­˜æ•°æ®ï¼Œä½¿ç”¨è¿‘ä¼¼è®¡ç®—
        created_at_str = repo_data.get("created_at", "")
        updated_at_str = repo_data.get("updated_at", "")

        try:
            created_at = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
            updated_at = datetime.fromisoformat(updated_at_str.replace("Z", "+00:00"))

            # é¡¹ç›®å­˜åœ¨å¤©æ•°
            days_since_creation = max(1, (datetime.now(updated_at.tzinfo) - created_at).days)

            # å¹³å‡æ¯å¤© star å¢é•¿æ•°
            star_velocity = current_stars / days_since_creation

            # æœ€è¿‘æ›´æ–°åŠ æˆ
            days_since_update = (datetime.now(updated_at.tzinfo) - updated_at).days
            recency_multiplier = 1.5 if days_since_update < 7 else 1.0

            # è¿‘ä¼¼å¢é•¿é€Ÿåº¦
            estimated_velocity = star_velocity * recency_multiplier

            print(f"  ğŸ“Š {repo_name}: ä¼°ç®—å¢é•¿ {estimated_velocity:.1f} stars/å¤© (åˆ›å»ºäº {days_since_creation} å¤©å‰)")

            return estimated_velocity

        except Exception as e:
            print(f"  âš ï¸  è®¡ç®— {repo_name} å¢é•¿é€Ÿåº¦å¤±è´¥: {e}")
            return 0

    def _calculate_relevance_score(self, repo_data: Dict) -> Tuple[float, List[str]]:
        """
        è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0-100ï¼‰ï¼Œå¹¶è¿”å›å‘½ä¸­çš„ä¸»é¢˜é€šé“
        """
        repo_name = (repo_data.get("full_name") or "").lower()
        description = (repo_data.get("description") or "").lower()
        topics = [topic.lower() for topic in repo_data.get("topics", [])]
        topics_text = " ".join(topics)

        term_rules = [
            ("vibecoding", ["vibecoding", "vibe coding"]),
            ("codex", ["codex", "openai codex"]),
            ("claude", ["claude", "claude code"]),
        ]

        score = 0.0
        matched_channels = []

        for channel, terms in term_rules:
            channel_matched = False
            for term in terms:
                term_score = 0.0
                if term in repo_name:
                    term_score += 40
                if term in topics_text:
                    term_score += 30
                if term in description:
                    term_score += 18

                if term_score > 0:
                    score += term_score
                    channel_matched = True
                    break

            if channel_matched:
                matched_channels.append(channel)

        if len(matched_channels) > 1:
            score += 10

        return min(score, 100.0), matched_channels

    def _calculate_growth_score(self, star_velocity: float) -> float:
        """
        å°† star å¢é€Ÿæ˜ å°„åˆ° 0-100 åˆ†
        """
        positive_velocity = max(0.0, star_velocity)
        capped_velocity = min(positive_velocity, 50.0)
        return (capped_velocity / 50.0) * 100.0

    def _calculate_quality_score(self, repo_data: Dict) -> float:
        """
        è®¡ç®—é¡¹ç›®å¥åº·åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰
        """
        updated_at_str = repo_data.get("updated_at", "")
        stars = repo_data.get("stargazers_count", 0)

        freshness_score = 30.0
        if updated_at_str:
            try:
                updated_at = datetime.fromisoformat(updated_at_str.replace("Z", "+00:00"))
                days_since_update = (datetime.now(updated_at.tzinfo) - updated_at).days
                if days_since_update <= 3:
                    freshness_score = 100.0
                elif days_since_update <= 7:
                    freshness_score = 85.0
                elif days_since_update <= 30:
                    freshness_score = 65.0
                elif days_since_update <= 90:
                    freshness_score = 45.0
                else:
                    freshness_score = 25.0
            except Exception:
                freshness_score = 30.0

        stability_score = (min(max(stars, 0), 5000) / 5000.0) * 100.0
        return freshness_score * 0.6 + stability_score * 0.4

    def _calculate_priority_score(
        self,
        relevance_score: float,
        growth_score: float,
        quality_score: float
    ) -> float:
        """
        è®¡ç®—ç»¼åˆä¼˜å…ˆçº§åˆ†æ•°
        """
        return (
            relevance_score * self.PRIORITY_WEIGHTS["relevance"] +
            growth_score * self.PRIORITY_WEIGHTS["growth"] +
            quality_score * self.PRIORITY_WEIGHTS["quality"]
        )

    def _extract_repo_info(
        self,
        repo_data: Dict,
        star_velocity: float,
        activity_score: float,
        relevance_score: float = 0.0,
        growth_score: float = 0.0,
        quality_score: float = 0.0,
        matched_channels: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        æå–ä»“åº“ä¿¡æ¯

        Args:
            repo_data: GitHub API è¿”å›çš„ä»“åº“æ•°æ®
            star_velocity: star å¢é•¿é€Ÿåº¦
            activity_score: æ´»è·ƒåº¦åˆ†æ•°

        Returns:
            ä»“åº“ä¿¡æ¯å­—å…¸
        """
        return {
            "name": repo_data["full_name"],
            "description": repo_data.get("description") or "æš‚æ— æè¿°",
            "url": repo_data["html_url"],
            "stars": repo_data.get("stargazers_count", 0),
            "forks": repo_data.get("forks_count", 0),
            "open_issues": repo_data.get("open_issues_count", 0),
            "language": repo_data.get("language") or "Unknown",
            "created_at": repo_data.get("created_at", ""),
            "updated_at": repo_data.get("updated_at", ""),
            "topics": repo_data.get("topics", []),
            "star_velocity": round(star_velocity, 2),
            "activity_score": round(activity_score, 2),
            "relevance_score": round(relevance_score, 2),
            "growth_score": round(growth_score, 2),
            "quality_score": round(quality_score, 2),
            "matched_channels": matched_channels or [],
        }

    def _merge_channels(self, channels_a: List[str], channels_b: List[str]) -> List[str]:
        """
        åˆå¹¶ä¸»é¢˜é€šé“å¹¶æŒ‰å›ºå®šé¡ºåºè¾“å‡ºï¼Œä¿è¯ç»“æœç¨³å®š
        """
        merged = set(channels_a).union(set(channels_b))
        ordered_channels = [channel for channel in self.CHANNEL_ORDER if channel in merged]
        others = sorted([channel for channel in merged if channel not in self.CHANNEL_ORDER])
        return ordered_channels + others

    def _select_channel_balanced_projects(self, all_projects: Dict[str, Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
        """
        å…ˆæŒ‰é€šé“å– TopNï¼Œå†åˆå¹¶å…¨å±€æ’åºï¼Œé¿å…å•ä¸€é€šé“å„æ–­
        """
        selected_projects: Dict[str, Dict[str, Any]] = {}

        for channel in self.CHANNEL_ORDER:
            channel_projects = [
                project for project in all_projects.values()
                if channel in project.get("matched_channels", [])
            ]
            channel_projects.sort(key=lambda project: project["activity_score"], reverse=True)
            for project in channel_projects[:self.CHANNEL_TOP_N]:
                selected_projects[project["name"]] = project

        if len(selected_projects) < limit:
            global_projects = list(all_projects.values())
            global_projects.sort(key=lambda project: project["activity_score"], reverse=True)
            for project in global_projects:
                if project["name"] not in selected_projects:
                    selected_projects[project["name"]] = project
                if len(selected_projects) >= limit:
                    break

        merged_projects = list(selected_projects.values())
        merged_projects.sort(key=lambda project: project["activity_score"], reverse=True)
        return merged_projects[:limit]

    def fetch_active_ai_projects(self, limit: int = 30) -> List[Dict[str, Any]]:
        """
        è·å–æ´»è·ƒçš„ AI é¡¹ç›®

        Args:
            limit: è·å–çš„é¡¹ç›®æ•°é‡

        Returns:
            é¡¹ç›®ä¿¡æ¯åˆ—è¡¨ï¼ŒæŒ‰æ´»è·ƒåº¦æ’åº
        """
        all_projects = {}

        # ç¬¬ä¸€é˜¶æ®µï¼šæŒ‰é€šé“å¬å›å€™é€‰é¡¹ç›®
        for channel in self.CHANNEL_ORDER:
            queries = self.SEARCH_CHANNELS.get(channel, [])
            print(f"ğŸ” æœç´¢é€šé“: {channel}")

            for query in queries:
                repos = self._search_repositories(query, per_page=20)
                print(f"  ğŸ“¦ æŸ¥è¯¢ç»“æœ {len(repos)} ä¸ª: {query}")

                for repo_data in repos:
                    # ä½¿ç”¨é¡¹ç›®åç§°ä½œä¸ºå”¯ä¸€æ ‡è¯†å»é‡
                    repo_name = repo_data["full_name"]

                    # è®¡ç®—åŸºç¡€åˆ†æ•°
                    star_velocity = self._calculate_star_velocity(repo_name, repo_data)
                    relevance_score, matched_channels = self._calculate_relevance_score(repo_data)
                    matched_channels = self._merge_channels(matched_channels, [channel])
                    growth_score = self._calculate_growth_score(star_velocity)
                    quality_score = self._calculate_quality_score(repo_data)
                    priority_score = self._calculate_priority_score(
                        relevance_score=relevance_score,
                        growth_score=growth_score,
                        quality_score=quality_score
                    )

                    if repo_name in all_projects:
                        # å·²å­˜åœ¨æ—¶åˆå¹¶é€šé“ï¼Œå¹¶ä¿ç•™æ›´é«˜åˆ†
                        merged_channels = self._merge_channels(
                            all_projects[repo_name].get("matched_channels", []),
                            matched_channels
                        )

                        if priority_score > all_projects[repo_name]["activity_score"]:
                            info = self._extract_repo_info(
                                repo_data=repo_data,
                                star_velocity=star_velocity,
                                activity_score=priority_score,
                                relevance_score=relevance_score,
                                growth_score=growth_score,
                                quality_score=quality_score,
                                matched_channels=merged_channels,
                            )
                            all_projects[repo_name] = info
                        else:
                            all_projects[repo_name]["matched_channels"] = merged_channels
                        continue

                    # æ–°é¡¹ç›®å†™å…¥å€™é€‰æ± 
                    info = self._extract_repo_info(
                        repo_data=repo_data,
                        star_velocity=star_velocity,
                        activity_score=priority_score,
                        relevance_score=relevance_score,
                        growth_score=growth_score,
                        quality_score=quality_score,
                        matched_channels=matched_channels,
                    )
                    all_projects[repo_name] = info

                    print(
                        f"  âœ“ è·å–: {info['name']} (ğŸ¯ {info['relevance_score']} | "
                        f"ğŸ“ˆ {info['growth_score']} | ğŸ§ª {info['quality_score']} | "
                        f"ğŸ“Š {info['activity_score']})"
                    )

        # ç¬¬äºŒé˜¶æ®µï¼šé€šé“ TopN åˆå¹¶åå…¨å±€æ’åº
        projects = self._select_channel_balanced_projects(all_projects, limit)

        # æ›´æ–°ç¼“å­˜ï¼ˆä¿å­˜æ‰€æœ‰è·å–åˆ°çš„é¡¹ç›®çš„ star æ•°ï¼‰
        print("\nğŸ’¾ æ›´æ–°ç¼“å­˜...")
        for project in projects:
            self.cache.update_stars(project["name"], project["stars"])
        self.cache.save()

        return projects[:limit]

    def get_readme_content(self, repo_name: str) -> str:
        """
        è·å–ä»“åº“çš„ README å†…å®¹

        Args:
            repo_name: ä»“åº“å…¨å (owner/repo)

        Returns:
            README å†…å®¹ (Markdown æ ¼å¼)
        """
        try:
            # å…ˆå°è¯•è·å– README æ–‡ä»¶
            response = self._http_get(
                f"{self.base_url}/repos/{repo_name}/readme"
            )
            if response is None:
                return ""

            if response.status_code == 200:
                data = response.json()
                # README å†…å®¹æ˜¯ base64 ç¼–ç çš„
                import base64
                content = base64.b64decode(data["content"]).decode("utf-8")
                return content
            else:
                return ""
        except Exception as e:
            print(f"  âš ï¸  è·å– README å¤±è´¥ {repo_name}: {e}")
            return ""
